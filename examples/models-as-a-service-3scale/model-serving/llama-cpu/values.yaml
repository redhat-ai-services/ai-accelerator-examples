# reference values.yaml:
# https://raw.githubusercontent.com/redhat-ai-services/helm-charts/refs/heads/main/charts/vllm-kserve/values.yaml

fullnameOverride: llama-32-1b-instruct-cpu

model:
  # -- Option to set how the storage will be configured.  Options: "uri" and "s3"
  mode: uri

  # -- The Uri to use for storage.  Mode must be set to "uri" to use this option.  Options: "oci://" and "pvc://"
  # More models available at: https://github.com/redhat-ai-services/modelcar-catalog
  uri: oci://quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-1b-instruct
  args:
    - --max-model-len=2000

deploymentMode: RawDeployment

endpoint:
  externalRoute:
    # -- Creates an externally accessible route for the model endpoint
    enabled: false

  auth:
    # -- Secures the model endpoint and creates a role to grant permissions to service accounts
    enabled: false

    # -- Creates service accounts with permissions to access the secured endpoint
    serviceAccounts:
      - name: llama-32-1b-instruct-cpu

image:
  image: quay.io/rh-aiservices-bu/vllm-cpu-openai-ubi9
  tag: "0.3"
  runtimeVersionOverride: 0.7.3

tolerations: []

resources:
  requests:
    cpu: '2'
    memory: 2Gi
    nvidia.com/gpu: null
  limits:
    cpu: '4'
    memory: 8Gi
    nvidia.com/gpu: null
